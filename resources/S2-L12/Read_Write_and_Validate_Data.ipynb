{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Writing and Validating Data in PySpark\n",
    "\n",
    "Welcome to PySpark!\n",
    "\n",
    "In this first lecture, we will be covering:\n",
    "\n",
    " - Reading in Data\n",
    " - Partioned Files\n",
    " - Validating Data\n",
    " - Specifying Data Types\n",
    " - Writing Data\n",
    "\n",
    "Below you will see the script to begin your first PySpark instance. If you're ever curious about how your PySpark instance is performing, Spark offers a neat Web UI with tons of information. Just navigate to http://[driver]:4040 in your browswer where \"drive\" is you driver name. If you are running PySpark locally, it would be http://localhost:4040 or you can use the hyperlink automatically produced from the script below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T07:56:53.264181400Z",
     "start_time": "2023-05-22T07:56:53.232888800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1c9713b6820>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>ReadWriteVal</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's create our PySpark instance!\n",
    "\n",
    "# PC users can use the next two lines of code but mac users don't need it\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark  # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"ReadWriteVal\").master(\"local[4]\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "\n",
    "A DataFrame is equivalent to a relational table in Spark SQL, and can be created using various functions in SparkSession.\n",
    "\n",
    "First let's try reading in a csv file containing a list of students and their grades.\n",
    "\n",
    "**Source:** https://www.kaggle.com/spscientist/students-performance-in-exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:03:58.636624Z",
     "start_time": "2023-05-22T08:03:51.789775600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start by reading a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "path = \"Datasets/\"\n",
    "\n",
    "# Some csv data\n",
    "students = spark.read.csv(path + 'students.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parquet Files**\n",
    "\n",
    "Now try reading in a parquet file. This is most common data type in the big data world.\n",
    "Why? because it is the most compact file storage method (even better than zipped files!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:13:46.112222Z",
     "start_time": "2023-05-22T08:13:44.968796100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|           email|gender|    ip_address|              cc|  country|birthdate|   salary|           title|comments|\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|2016-02-03 08:55:29|  1|    Amanda|   Jordan|ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|Internal Auditor|   1E+02|\n",
      "|2016-02-03 18:04:03|  2|    Albert|  Freeman| afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|   Accountant IV|        |\n",
      "+-------------------+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet = spark.read.parquet(path + 'users1.parquet')\n",
    "parquet.show(2)\n",
    "parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partitioned Parquet Files**\n",
    "\n",
    "Actually most big datasets will be partitioned. Here is how you can collect all the pieces (parts) of the dataset in one simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:28:04.531448Z",
     "start_time": "2023-05-22T08:28:03.826253400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "| id|first_name|last_name|           email|gender|    ip_address|              cc|  country|birthdate|   salary|           title|comments|\n",
      "+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "|  1|    Amanda|   Jordan|ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|Internal Auditor|   1E+02|\n",
      "|  2|    Albert|  Freeman| afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|   Accountant IV|        |\n",
      "+---+----------+---------+----------------+------+--------------+----------------+---------+---------+---------+----------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "   id first_name last_name                     email  gender      ip_address   \n0   1     Amanda    Jordan          ajordan0@com.com  Female     1.197.201.2  \\\n1   2     Albert   Freeman           afreeman1@is.gd    Male  218.111.175.34   \n2   3     Evelyn    Morgan   emorgan2@altervista.org  Female    7.161.136.94   \n3   4     Denise     Riley          driley3@gmpg.org  Female   140.35.109.83   \n4   5     Carlos     Burns  cburns4@miitbeian.gov.cn          169.113.235.40   \n\n                 cc       country  birthdate     salary   \n0  6759521864920116     Indonesia   3/8/1971   49756.53  \\\n1                          Canada  1/16/1968  150280.17   \n2  6767119071901597        Russia   2/1/1960  144972.51   \n3  3576031598965625         China   4/8/1997   90263.05   \n4  5602256255204850  South Africa                   NaN   \n\n                    title comments  \n0        Internal Auditor    1E+02  \n1           Accountant IV           \n2     Structural Engineer           \n3  Senior Cost Accountant           \n4                                   ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>first_name</th>\n      <th>last_name</th>\n      <th>email</th>\n      <th>gender</th>\n      <th>ip_address</th>\n      <th>cc</th>\n      <th>country</th>\n      <th>birthdate</th>\n      <th>salary</th>\n      <th>title</th>\n      <th>comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Amanda</td>\n      <td>Jordan</td>\n      <td>ajordan0@com.com</td>\n      <td>Female</td>\n      <td>1.197.201.2</td>\n      <td>6759521864920116</td>\n      <td>Indonesia</td>\n      <td>3/8/1971</td>\n      <td>49756.53</td>\n      <td>Internal Auditor</td>\n      <td>1E+02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Albert</td>\n      <td>Freeman</td>\n      <td>afreeman1@is.gd</td>\n      <td>Male</td>\n      <td>218.111.175.34</td>\n      <td></td>\n      <td>Canada</td>\n      <td>1/16/1968</td>\n      <td>150280.17</td>\n      <td>Accountant IV</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Evelyn</td>\n      <td>Morgan</td>\n      <td>emorgan2@altervista.org</td>\n      <td>Female</td>\n      <td>7.161.136.94</td>\n      <td>6767119071901597</td>\n      <td>Russia</td>\n      <td>2/1/1960</td>\n      <td>144972.51</td>\n      <td>Structural Engineer</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Denise</td>\n      <td>Riley</td>\n      <td>driley3@gmpg.org</td>\n      <td>Female</td>\n      <td>140.35.109.83</td>\n      <td>3576031598965625</td>\n      <td>China</td>\n      <td>4/8/1997</td>\n      <td>90263.05</td>\n      <td>Senior Cost Accountant</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Carlos</td>\n      <td>Burns</td>\n      <td>cburns4@miitbeian.gov.cn</td>\n      <td></td>\n      <td>169.113.235.40</td>\n      <td>5602256255204850</td>\n      <td>South Africa</td>\n      <td></td>\n      <td>NaN</td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "partitioned = spark.read.parquet(path + 'users*').withColumn(\n",
    "    \"registration_dttm\", col(\"registration_dttm\").cast(\"timestamp\")\n",
    ").drop(\"registration_dttm\")\n",
    "partitioned.show(2)\n",
    "partitioned.count()\n",
    "partitioned.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also opt to read in only a specific set of paritioned parquet files. Say for example that you only wanted users1 and users2 and not users3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:32:01.649248600Z",
     "start_time": "2023-05-22T08:32:01.444047800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|2016-02-03 08:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
      "|2016-02-03 18:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
      "|2016-02-03 02:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
      "|2016-02-03 01:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
      "|2016-02-03 06:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     null|                    |                    |\n",
      "|2016-02-03 08:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
      "|2016-02-03 09:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
      "|2016-02-03 07:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
      "|2016-02-03 04:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
      "|2016-02-03 19:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
      "|2016-02-03 01:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
      "|2016-02-03 19:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
      "|2016-02-03 19:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
      "|2016-02-03 22:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
      "|2016-02-03 09:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
      "|2016-02-03 01:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
      "|2016-02-03 01:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
      "|2016-02-03 17:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
      "|2016-02-03 12:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
      "|2016-02-03 11:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that the .option(\"basePath\", path) option is used to override the automatic function\n",
    "# that will exclude the partitioned variable in resulting dataframe. \n",
    "# I prefer to have the partitioning info in my new dataframe personally. \n",
    "users1_2 = spark.read.option(\"basePath\", path).parquet(path + 'users1.parquet', path + 'users2.parquet')\n",
    "users1_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you're in AWS storing data in s3 buckets your code will more like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o52.parquet.\n: java.io.IOException: No FileSystem for scheme: s3\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:644)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-7cd2afe36921>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mkey3\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"partition_test/Table1/CREATED_YEAR=2018/*\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mtest_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m's3://'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mbucket\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m'/'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mkey1\u001B[0m\u001B[1;33m,\u001B[0m                             \u001B[1;34m's3://'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mbucket\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m'/'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mkey2\u001B[0m\u001B[1;33m,\u001B[0m                              \u001B[1;34m's3://'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mbucket\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;34m'/'\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0mkey3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mtest_df\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[1;34m(self, *paths)\u001B[0m\n\u001B[0;32m    314\u001B[0m         \u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'name'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'string'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m'year'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'int'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m'month'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'int'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m'day'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'int'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    315\u001B[0m         \"\"\"\n\u001B[1;32m--> 316\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_to_seq\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpaths\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    317\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mignore_unicode_prefix\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[1;32m-> 1257\u001B[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[0;32m   1258\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1259\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001B[0m in \u001B[0;36mdeco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m     61\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[0;32m    327\u001B[0m                     \u001B[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 328\u001B[1;33m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[0;32m    329\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o52.parquet.\n: java.io.IOException: No FileSystem for scheme: s3\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:644)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "bucket = \"my_bucket\"\n",
    "key1 = \"partition_test/Table1/CREATED_YEAR=2015/*\"\n",
    "key2 = \"partition_test/Table1/CREATED_YEAR=2017/*\"\n",
    "key3 = \"partition_test/Table1/CREATED_YEAR=2018/*\"\n",
    "\n",
    "test_df = spark.read.parquet('s3://' + bucket + '/' + key1, \\\n",
    "                             's3://' + bucket + '/' + key2, \\\n",
    "                             's3://' + bucket + '/' + key3)\n",
    "\n",
    "test_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Data\n",
    "\n",
    "Next you will want to validate that you dataframe was read in correct. We will get into more detailed data evaluation later on but first we need to ensure that all the variable types were infered correctly and that the values actually made it in... sometimes they don't :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:35:45.466665900Z",
     "start_time": "2023-05-22T08:35:45.261520100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+---------------------------+--------+-----------------------+----------+-------------+-------------+\n",
      "|gender|race/ethnicity|parental level of education|   lunch|test preparation course|math score|reading score|writing score|\n",
      "+------+--------------+---------------------------+--------+-----------------------+----------+-------------+-------------+\n",
      "|female|       group B|          bachelor's degree|standard|                   none|        72|           72|           74|\n",
      "|female|       group C|               some college|standard|              completed|        69|           90|           88|\n",
      "|female|       group B|            master's degree|standard|                   none|        90|           95|           93|\n",
      "+------+--------------+---------------------------+--------+-----------------------+----------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get an inital view of your dataframe\n",
    "students.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  \n",
       "4                    none          76             78             75  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your dataframe is more than just a few variables, this method is way better\n",
    "students.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:36:38.618930Z",
     "start_time": "2023-05-22T08:36:38.425885500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Note the types here:\n",
    "print(type(students))\n",
    "studentsPdf = students.toPandas()\n",
    "print(type(studentsPdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:36:59.669264600Z",
     "start_time": "2023-05-22T08:36:58.535502600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gender: string (nullable = true)\n",
      " |-- race/ethnicity: string (nullable = true)\n",
      " |-- parental level of education: string (nullable = true)\n",
      " |-- lunch: string (nullable = true)\n",
      " |-- test preparation course: string (nullable = true)\n",
      " |-- math score: integer (nullable = true)\n",
      " |-- reading score: integer (nullable = true)\n",
      " |-- writing score: integer (nullable = true)\n",
      "\n",
      "None\n",
      "\n",
      "['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'math score', 'reading score', 'writing score']\n",
      "\n",
      "DataFrame[summary: string, gender: string, race/ethnicity: string, parental level of education: string, lunch: string, test preparation course: string, math score: string, reading score: string, writing score: string]\n"
     ]
    }
   ],
   "source": [
    "# A Solid Summary of your data:\n",
    "\n",
    "#show the data (like df.head())\n",
    "print(students.printSchema())\n",
    "print(\"\")\n",
    "print(students.columns)\n",
    "print(\"\")\n",
    "print(students.describe())  # Not so fond of this one but to each their own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:40:21.336911Z",
     "start_time": "2023-05-22T08:40:21.289525900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "IntegerType"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you need to get the type of just ONE column by name you can use this function:\n",
    "students.schema['math score'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:40:23.850488900Z",
     "start_time": "2023-05-22T08:40:23.504802400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        math score|\n",
      "+-------+------------------+\n",
      "|  count|              1000|\n",
      "|   mean|            66.089|\n",
      "| stddev|15.163080096009454|\n",
      "|    min|                 0|\n",
      "|    max|               100|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neat \"describe\" function\n",
    "students.describe(['math score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:47:07.669849Z",
     "start_time": "2023-05-22T08:47:07.416045100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+-------------+\n",
      "|summary|math score|reading score|writing score|\n",
      "+-------+----------+-------------+-------------+\n",
      "|  count|      1000|         1000|         1000|\n",
      "|    min|         0|           17|           10|\n",
      "|    25%|        57|           59|           57|\n",
      "|    75%|        77|           79|           79|\n",
      "|    max|       100|          100|          100|\n",
      "+-------+----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary function\n",
    "students.select(\"math score\", \"reading score\", \"writing score\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to specify data types as you read in datasets.\n",
    "\n",
    "Some data types make it easier to infer schema (like tabular formats such as csv which we will show later). \n",
    "\n",
    "However you often have to set the schema yourself if you aren't dealing with a .read method that doesn't have inferSchema() built-in.\n",
    "\n",
    "Spark has all the tools you need for this, it just requires a very specific structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:49:31.567090800Z",
     "start_time": "2023-05-22T08:49:31.539856200Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create the list of Structure fields\n",
    "    * :param name: string, name of the field.\n",
    "    * :param dataType: :class:`DataType` of the field.\n",
    "    * :param nullable: boolean, whether the field can be null (None) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:49:33.720713600Z",
     "start_time": "2023-05-22T08:49:33.687543600Z"
    }
   },
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"name\", StringType(), True),\n",
    "               StructField(\"email\", StringType(), True),\n",
    "               StructField(\"city\", StringType(), True),\n",
    "               StructField(\"mac\", StringType(), True),\n",
    "               StructField(\"timestamp\", DateType(), True),\n",
    "               StructField(\"creditcard\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:49:36.277041800Z",
     "start_time": "2023-05-22T08:49:36.240459700Z"
    }
   },
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a .json file this time :) \n",
    "\n",
    "**Source:** https://gist.github.com/raine/da15845f332a2fb8937b344504abfbe0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:51:22.786090800Z",
     "start_time": "2023-05-22T08:51:22.706766Z"
    }
   },
   "outputs": [],
   "source": [
    "people = spark.read.json(path + 'people.json', schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:51:23.775967900Z",
     "start_time": "2023-05-22T08:51:23.726189100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- mac: string (nullable = true)\n",
      " |-- timestamp: date (nullable = true)\n",
      " |-- creditcard: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "First let's just try writing a simple csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:52:16.742035Z",
     "start_time": "2023-05-22T08:52:15.709111400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note the funky naming convention of the file in your output folder. There is no way to directly change this. \n",
    "students.write.mode(\"overwrite\").csv('write_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the strange naming convention of the output file in the path that you specified. Spark uses Hadoop File Format, which requires data to be partitioned - that's why you have part- files. If you want to rename your written files to a more user friendly format, you can do that using the method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:55:50.821033900Z",
     "start_time": "2023-05-22T08:55:50.758512400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "file = fs.globStatus(spark._jvm.Path('write_test.csv/part*'))[0].getPath().getName()\n",
    "fs.rename(spark._jvm.Path('write_test.csv/' + file),\n",
    "          spark._jvm.Path('write_test2.csv'))  #these two need to be different\n",
    "fs.delete(spark._jvm.Path('write_test.csv'), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writting Parquet files\n",
    "\n",
    "Now let's try writing a parquet file. This is best practice for big data as it is the most compact storage method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T09:02:00.173702800Z",
     "start_time": "2023-05-22T09:01:59.248567900Z"
    }
   },
   "outputs": [],
   "source": [
    "users1_2.write.mode(\"overwrite\").parquet('parquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who got an error attempting to run the above code. Try this solution: https://stackoverflow.com/questions/59220832/unable-to-write-spark-dataframe-to-a-parquet-file-format-to-c-drive-in-pyspark\n",
    "\n",
    "#### Writting Partitioned Parquet Files\n",
    "\n",
    "Now try to write a partioned parquet file... super fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T09:02:26.255681800Z",
     "start_time": "2023-05-22T09:02:24.948910200Z"
    }
   },
   "outputs": [],
   "source": [
    "users1_2.write.mode(\"overwrite\").partitionBy(\"gender\").parquet('part_parquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writting your own dataframes here!\n",
    "\n",
    "You can also create your own dataframes directly here in your Juypter Notebook too if you want. \n",
    "\n",
    "Like this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T09:02:59.805337300Z",
     "start_time": "2023-05-22T09:02:51.373888300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|     fruit|quantity|\n",
      "+----------+--------+\n",
      "|      Pear|      10|\n",
      "|    Orange|      36|\n",
      "|    Banana|     123|\n",
      "|      Kiwi|      48|\n",
      "|     Peach|      16|\n",
      "|Strawberry|       1|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values = [('Pear', 10), ('Orange', 36), ('Banana', 123), ('Kiwi', 48), ('Peach', 16), ('Strawberry', 1)]\n",
    "df = spark.createDataFrame(values, ['fruit', 'quantity'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it! Great job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
